* Add a script to build SoFCheck with profile-guided optimizations (aka PGO).
  Profile-guided optimizations may give us some speed improvement. But building with PGO is not
  that simple, as it requires two builds (one build to gather data and one build to perform the
  optimizations based on that data). Ideally, we want a script in tools/ that is able to do all
  the building work (create two directories, gather information on the first build, then invoke
  the second build).

* Make UCI options case-insensitive
  UCI standard says than UCI options are case-insensitive, but they are case-sensitive in the
  current implementation. We need to patch the `OptionStorage` class and all the observers that
  check for option name. Do not forget to update UCI name escaping and baseline tests for UCI.
  Better verify all the usages of UCI options whether they are still correct. Finally, state in
  the docs that the options are now case-insensitive.

* Write stricter `ValidateBoard`
  Add more heuristics to check that the board is theoretically possible, i.e no more that 9 queens,
  10 knights, 10 bishops etc. Now we check only for one king and no more than 16 pieces. This may
  be good as it makes impossible to create a position in which the quiescense search runs for too
  long. (Currently, the position with 15 rooks on both sides may hang the engine.) The downside is
  as follows: some chess GUIs may accept the positions which won't pass our `ValidateBoard`, and
  will think that our engine behaves incorrenly in such case. Another idea for board validation is
  to prevent triple and impossible checks. Chess package for Python does this, see its sources:
  https://github.com/niklasf/python-chess/blob/v1.6.1/chess/__init__.py#L3282. Maybe we can invent
  even more heuristics.

* Add the functions to replace `std::from_chars` and `std::to_chars`
  These two functions are declared in `<charconv>` and exist in C++17, but not every compiler
  supports it. SoFCheck may gain support of older versions of Clang and GCC if we don't use the
  `<charconv>` header on these platforms and use our hand-written functions instead. Now the issue
  exists, as Ubuntu 18.04 uses GCC 7 as default compiler. But if it won't be fixed in the next
  several years, we may simply consider that GCC 7 is old enough and unsupported, and just drop
  this issue.

* Rethink the Client/ServerConnector architecture
  The architecture where each `Client` is connected to `ServerConnector` and each `Server` is
  connected to `ClientConnector` is good now, but has some limitations which may arise in the
  future. For instance, we may write an engine adapter, which will allow to launch an engine
  without creating a `Server` instance. In this case it's not good to write a `ServerConnector`,
  as the interactions with the client will be outside of `poll()` method, so implementing `poll()`
  would be useless. Another thing worth rethinking is thread-safety. Currently, `Client` and
  `Server` are not thread-safe, while `ClientConnector` and `ServerConnector` are thread-safe.
  When we get rid of the current architecture, we will need another approach to define what must
  be thread-safe.

* Ensure that the transposition table doesn't change search results
  To do this, we need to perform a simple test. Get the score from the transposition table only
  when the depth is equal to our depth, not greater-or-equal. Then, compare the results with this
  modification of transposition table and without any transposition table. They must be equal. The
  test must be performed only one time, as it will be unable to perform it in the future, when many
  other heuristics will appear.

* Do not analyze the same node twice in multiple threads
  To do this, we may put a flag `busy` into the hash table, which indicates that this position is
  already being analysed. If we encounter a busy position during the search, we just skip it and
  say that we'll analyse it in the end. Such trick may improve performance of the engine in the
  multithreaded case. This idea is used in Texel chess engine, so we may also try it.

* Improve tooling for datasets used for weights tuning
  The framework to extract and apply weights from the engine can be considered well-written and
  convenient. But the tooling used to create datasets for training and apply new weights into the
  engine is not. To improve the situation, we can do the following. First, we need to rethink the
  dataset file format coming from Battlefield. Ideally, it must output moves, not FENs, so the
  resulting file will be more compact and easier to work with. The old dataset files located in
  `sofcheck-weights-tuning` must be converted. Second, we need to create a single library to work
  with dataset files and use it to write the utilities for manipulating datasets. The old Python
  script `concat_datasets.py` must be removed. Third, we may use the idea similar to Zurichess
  tuner (see https://bitbucket.org/zurichess/tuner) and pass only a random subset of positions from
  `make_dataset`, not all of them. Fourth, we may create wrappers for the tools `make_dataset`,
  `apply_weights` and `show_weights`, to allow omitting the features JSON file location in the
  command-line arguments. These wrappers must be generated by CMake at build time in the build
  directory.
